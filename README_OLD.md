# skills_test_3

## Author: Paul A. Wambo
This is my solution to the Skill Test Challenge for the open Bioinformatics Specialist position within the CERC at McGill University.

## System Requirements
Ubuntu 18.04/22.04 or Debian 11

## Folder Description
- data
    - Input: Files provided for the challenge (*.cram, *.cram.crai, *.txt)
    - Output: Where files generated by the pipeline would be stored
- reference: Reference genome (GRCh38) and its index
- VerifyBamID: Build version of VerifyBamID2
- example: Provided output of the coding challenge used for guidance
- pipeline:
    - config:
        - config.yaml
    - workflow:
        - notebooks: Jupyter Notebook for analysis and data wrangling
        - Snakefile: Snakemake pipeline logic
    - run_test.sh: Test to determine if the pipeline compiles
    - run.sh: Runs pipeline
- .gitignore
- cerc-skill-test.txt: Requirements files to reproduce conda environment
- ReadMe.md

## Reproduce Environments
If you do not have Anaconda, please install it using the following directives: `https://linuxize.com/post/how-to-install-anaconda-on-ubuntu-20-04/``
Then create a conda environment using the following command: `conda env create --file cerc-skill-test.txt`

## Problem description

Given sequencing data for 10 study individuals in CRAM file format (one file per individual) inside `input/` directory, implement an automatic workflow which for each indiviual:
1. **Estimates DNA contamination and genetic ancestry**. To estimate DNA contamination levels and genetic ancestry, use the [VerifyBamID](https://github.com/Griffan/VerifyBamID) tool and 1000 Genome Project (1000g) GRCh38 reference panel, which is provided together with the tool. Specify 4 Principal Components (PC) with the `--NumPC` option, e.g.:
    
    ```
    VerifyBamID.Linux.x86-64 --SVDPrefix ${VERIFY_BAM_ID_HOME}/resource/1000g.phase3.100k.b38.vcf.gz.dat --Reference GRCh38_full_analysis_set_plus_decoy_hla.fa --BamFile `input/HGDP00082.GRCh38.low_coverage.cram` --NumPC 4 ...
    ``` 
    The human genome reference file and its index can be downloaded from
    ```
    ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa
    ```
    and
    ```
    ftp://ftp-trace.ncbi.nih.gov/1000genomes/ftp/technical/reference/GRCh38_reference_genome/GRCh38_full_analysis_set_plus_decoy_hla.fa.fai
    ```
    , correspondingly.
    
    This step will generate `*.Ancestry` and `*.selfSM` output files for each study individual. The DNA contamination values are stored in `*.selfSM` files in the *FREEMIX* column. Create a new output file `Contamination.txt`, which stores a table with two tab-separated columns: SAMPLE -- sample name (i.e. HGDP00082, HGDP00450, ...), FREEMIX -- DNA contamination value from *FREEMIX* column in the corresponding `*.selfSM` file. You can find an example of the output file in the `example/` folder.

----------------------------------------------------------------
** Solution **

For each of the input files, the snakemake pipeline produces a `.Ancestry` and `.selfSM` file, then The `Contamination.txt` file is then produced by aggregating their results.
To generate these files, run the following code in your conda environment: `bash run.sh <num_cpus>`

----------------------------------------------------------------

2. **Visualizes estimated ancestry PCs**. The `${VERIFY_BAM_ID_HOME}/resource/1000g.phase3.100k.b38.vcf.gz.dat.V` file stores pre-computed 4 PCs for each individual in the 1000g reference panel, the `input/1000G_reference_populations.txt` file stores population labels (EUR, EAS, AMR, SAS, and AFR) for each individual in the 1000g reference panel, and the `*.Ancestry` files from the (1) workflow step store estimated PCs for 10 study individuals (*IntendedSample* column). Create the following 4 scatter plots visualizing reference individuals and 10 study individuals in the same space: PC1 vs PC2, PC2 vs PC3, PC3 vs PC4, and PC1 vs PC2 vs PC3 (i.e. 3 dimensional plot). Color reference individuals by their population labels (see plots in the `example/` folder). 

----------------------------------------------------------------
** Solution **

The pipeline first produces a file `data/output/all_estimated_pcs.txt`. This contains the sample ID, with its four associated PCs generated within the aggregated `.Ancestry` files.
A series of data wrangling steps are performed:
    1. Merging both reference files `${VERIFY_BAM_ID_HOME}/resource/1000g.phase3.100k.b38.vcf.gz.dat.V` and `input/1000G_reference_populations.txt` by the SAMPLE column.
        This allows us to maintain data integrity when associating each data point with the correct ancestry.
    2. Rearrange the data frame obtained from `data/output/all_estimated_pcs.txt`, to be in the format `SAMPLE, PC1, PC2, PC3, PC4` and setting the Ancestry to `Study`
    3. Combine both the reference and study data frames and make the series of required plots

----------------------------------------------------------------

3. **Assigns most likely super population**. Using population labels of 1000g reference panel individuals and their PC coordinates (stored in `input/1000G_reference_populations.txt` and `${VERIFY_BAM_ID_HOME}/resource/1000g.phase3.100k.b38.vcf.gz.dat.V`, correspondingly), assign most likely population label for each study individual. Save the assigned population labels in the `Populations.txt` file with the following 6 tab-separated columns: SAMPLE -- sample name (i.e. HGDP00082, HGDP00450, ...), PC1 - 1st PC from the *IntendedSample* column in the corresponding `*.Ancestry` file, PC2 - 2nd PC from the *IntendedSample* column in the corresponding `*.Ancestry` file, PC3 - 3rd PC from the *IntendedSample* column in the corresponding `*.Ancestry` file, PC4 - 4th PC from the *IntendedSample* column in the corresponding `*.Ancestry` file, 'POPULATION' - assigned population label (i.e. EUR, EAS, AMR, SAS, and AFR). You can find an example of the output file in the `example/` folder.

-------------------------------------------------------------------

** Solution **
1. Merging both reference files `${VERIFY_BAM_ID_HOME}/resource/1000g.phase3.100k.b38.vcf.gz.dat.V` and `input/1000G_reference_populations.txt` by the SAMPLE column.
    This allows us to maintain data integrity when associating each data point with the correct ancestry.
2. Use a Logistic Regression model to set a baseline. This allows to determine the level of poor performance we can accept, by determine classification performance i.e accuracy, balanced accuracy etc.
3. Train a Random Forest Classifier to get better results, as tree based models outperform most models on tabular data. Determine classification performance.
4. Perform hyperparameter tuning on the random forest classifier, using a grid search., and get the best model
5. Train the best suggested model, and predict the ancestry of the study participants
-------------------------------------------------------------------

## Requirements

To implement this workflow you may:
- use any workflow system of your choice, which is compatible with HPC (e.g. [Nextflow](https://www.nextflow.io), [Snakemake](https://snakemake.readthedocs.io/en/stable/), [Luigi](https://github.com/spotify/luigi), [Cromwell](https://cromwell.readthedocs.io/en/stable/))
- use any existing published open source software tools and libraries (e.g. R and Python libraries for plotting such as [matplotlib](https://matplotlib.org/) and [ggplot2](https://ggplot2.tidyverse.org/))
- use any scripting/programming language (or any combination of them) of your choice (e.g. Python, C/C++, Java, Perl, R, shell scripting)

## Solution

Please, send us a single compressed archive which includes:
1. **README**. It should provide: (a) a list of all open-source software tools (and their versions) which were used; (b) any additional requirements for the operating system and/or system libraries; (c) any compilation instructions if such exists (d) detailed step-by-step description on how to run the tool.
2. **Source of your scripts/code**. Please include detailed comments in your source code. This will help us better understand your code.
3. **Contamination.txt**. File generated by the 1st step in the workflow and which stores estimated DNA contamination values for 10 study individuals.
4. **Plots**. 4 plots (`*.png` or `.jpeg` format) generated by the 2nd step in the workflow (i.e. PC1 vs PC2, PC2 vs PC3, PC3 vc PC4, and PC1 vs PC2 vs PC3).
5. **Populations.txt**. File generated by the 3rd step in the workflow and which stores estimated PC coordinates and assigned population labels for 10 study individuals.


## Evaluation

The following will be evaluated:
1. Workflow can be easily installed and run.
2. `Contamination.txt` and `Populations.txt` files have all requested fields and their values are correct.
4. Plots are readable (e.g. have adequate axis labels and scaled properly, colors are distinguishable, 10 study individuals can be clearly distinguished from the reference individuals).
